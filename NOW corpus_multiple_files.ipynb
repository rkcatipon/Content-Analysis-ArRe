{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print ('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following files will be zipped:\n",
      "./2016-12_news_df.pkl\n",
      "./2016-11_news_df.pkl\n",
      "All files zipped successfully!\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile \n",
    "import os \n",
    "  \n",
    "def get_all_file_paths(directory): \n",
    "  \n",
    "    # initializing empty file paths list \n",
    "    file_paths = [] \n",
    "  \n",
    "    # crawling through directory and subdirectories \n",
    "    for root, directories, files in os.walk(directory): \n",
    "        for filename in files: \n",
    "            # join the two strings in order to form the full filepath. \n",
    "            if ('news_df') in filename and ('16') in filename:\n",
    "                filepath = os.path.join(root, filename) \n",
    "                file_paths.append(filepath) \n",
    "  \n",
    "    # returning all file paths \n",
    "    return file_paths         \n",
    "  \n",
    "def main(): \n",
    "    # path to folder which needs to be zipped \n",
    "    directory = './'\n",
    "  \n",
    "    # calling function to get all file paths in the directory \n",
    "    file_paths = get_all_file_paths(directory) \n",
    "  \n",
    "    # printing the list of all files to be zipped \n",
    "    print('Following files will be zipped:') \n",
    "    for file_name in file_paths: \n",
    "        print(file_name) \n",
    "  \n",
    "    # writing files to a zipfile \n",
    "    with ZipFile('2016.zip','w') as zip: \n",
    "        # writing each file one by one \n",
    "        for file in file_paths: \n",
    "            zip.write(file) \n",
    "  \n",
    "    print('All files zipped successfully!')\n",
    "main()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lucem_illud_2020 #pip install -U git+git://github.com/Computational-Content-Analysis-2020/lucem_illud_2020.git\n",
    "import pandas #For DataFrames\n",
    "import numpy as np #For arrays\n",
    "import pickle #if you want to save layouts\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import sys\n",
    "import spacy\n",
    "import time\n",
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final\n",
    "def loadDavies(address, fname, corpus_style=\"text\", num_files=10000, return_raw=True):\n",
    "    texts_raw = {}\n",
    "    article_count=0\n",
    "    file=address+'/'+fname\n",
    "    if corpus_style in file:\n",
    "                print(type(file),file)\n",
    "                zfile = zipfile.ZipFile(file)\n",
    "                for file in zfile.namelist():\n",
    "                    #check for us source\n",
    "                    if file.lower().find('us')==-1:\n",
    "                        continue\n",
    "                    texts_raw[file] = []\n",
    "                    with zfile.open(file) as f:\n",
    "                        for article in f:\n",
    "                            article_lower = article.decode().lower()\n",
    "                            #only take articles with election in them\n",
    "                            finde = 'election' in article_lower\n",
    "                            if (finde):\n",
    "                                article_count+=1\n",
    "                                texts_raw[file].append(article)\n",
    "    print (article_count)                            \n",
    "    if return_raw:\n",
    "        return(texts_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text-16-12.zip', 'text-16-11.zip']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "corpus_name = \"/project2/soci40133/Davies_corpora/NOW\"\n",
    "flist=os.listdir(corpus_name + \"/\")\n",
    "s_dict={}\n",
    "for f in flist:\n",
    "    #if ('text-18' in f or 'text-16-10' in f or 'text-16-11' in f or 'text-17' in f )and 'lexicon' not in f and'wlp' not in f and 'db' not in f and not 'text-18-11' in f and not 'text-18-12' in f:\n",
    "    if ('text-16-11' in f or 'text-16-12' in f):\n",
    "        s_dict[f]=f.replace(\"text\",\"sources\")\n",
    "\n",
    "print(list(s_dict.keys()))\n",
    "print(len(s_dict.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:get rid of everything before the first p tag and store headline seperately\n",
    "def clean_raw_text(raw_articles):\n",
    "    clean_articles = []\n",
    "    for text in raw_articles:\n",
    "        try:\n",
    "            text = text.decode(\"utf-8\")\n",
    "            clean_article = text.replace(\" \\'m\", \"'m\").replace(\" \\'ll\", \"'ll\").replace(\" \\'re\", \"'re\").replace(\" \\'s\", \"'s\").replace(\" \\'re\", \"'re\").replace(\" n\\'t\", \"n't\").replace(\" \\'ve\", \"'ve\").replace(\" /'d\", \"'d\")\\\n",
    "                            .replace(\"<h>\",\"\").replace(\"<p>\",\"\").replace(\"@@\",\"\")\n",
    "            clean_articles.append(clean_article)\n",
    "        except AttributeError:\n",
    "            # print(\"ERROR CLEANING\")\n",
    "            # print(text)\n",
    "            continue\n",
    "        except UnicodeDecodeError:\n",
    "            # print(\"Unicode Error, Skip\")\n",
    "            continue\n",
    "    return clean_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(text, remove_stop_words=False):\n",
    "    tokenized = []\n",
    "    customize_stop_words = ['is']\n",
    "    # pass word list through language model.\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    for w in customize_stop_words:\n",
    "        nlp.vocab[w].is_stop = True\n",
    "    for token in doc:\n",
    "        if not token.is_punct and len(token.text.strip()) > 0:\n",
    "            if remove_stop_words:\n",
    "                if not token.is_stop: \n",
    "                   tokenized.append(token.text)\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                tokenized.append(token.text)\n",
    "    \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_tokenize('Mr. aruna is bad', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Process(news_raw,file_name,source_name):\n",
    "    t0 = time.time()\n",
    "    ta = time.time()\n",
    "    a_count =0\n",
    "    news_texts = {}\n",
    "    just_texts = {}\n",
    "    for file in news_raw:\n",
    "        print('File_name',file)\n",
    "        #article=news_raw[file]\n",
    "        articles = clean_raw_text(news_raw[file])\n",
    "        article_cleaned=articles[0]\n",
    "        for article in articles:\n",
    "            if (a_count%100)==0:\n",
    "                t=time.time()\n",
    "                print(t-ta)\n",
    "                ta=time.time()\n",
    "                print('Processed',a_count)\n",
    "\n",
    "            ##where we sentence break, tokenize sentences, flatten, etc.\n",
    "            tokenized_sents = [lucem_illud_2020.word_tokenize(s) for s in lucem_illud_2020.sent_tokenize(article)]\n",
    "            normalized_sents = [lucem_illud_2020.normalizeTokens(s) for s in tokenized_sents]\n",
    "            #remove empty lists\n",
    "            normalized_sents =  list(filter(None, normalized_sents))\n",
    "            #print(normalized_sents)\n",
    "            tokens = [item for sublist in normalized_sents for item in sublist]\n",
    "            tid=article.split(\" \")[0]\n",
    "            #print(tid)\n",
    "            #print(tokens)\n",
    "            #tokens_spacy= word_tokenize(article)\n",
    "            try:\n",
    "                if tid in list(news_texts.values()):\n",
    "                    print (tid, 'already seen...bad coding')\n",
    "                news_texts[tid] = (tokens,normalized_sents)\n",
    "                just_texts[tid] = article\n",
    "                a_count+=1\n",
    "            except IndexError:\n",
    "                print(\"Missing: \",txts[0][2:])\n",
    "                continue\n",
    "    t1 = time.time()\n",
    "    print (\"Total time\", t1-t0)   \n",
    "#     try:\n",
    "#         assert(len(list(news_texts.keys()))==a_count)\n",
    "#     except:\n",
    "#         print(len(list(news_texts.keys())))\n",
    "#         print(a_count)\n",
    "    ids=set((news_texts.keys()))\n",
    "    sources = []\n",
    "    zfile = zipfile.ZipFile(corpus_name + \"/\"+source_name)\n",
    "    for file in zfile.namelist():\n",
    "        with zfile.open(file) as f:\n",
    "            for line in f:\n",
    "                sources.append(line)\n",
    "    sl=[]\n",
    "    ecount=0\n",
    "    for s in sources:\n",
    "        try:\n",
    "            sl.append(s.decode().split(\"\\t\")[0].strip())\n",
    "            #print(s.decode().split(\"\\t\")[2].strip())\n",
    "        except UnicodeDecodeError:\n",
    "            ecount+=1\n",
    "            continue\n",
    "    print(\"Total source documents\", len(sources))\n",
    "    print(\"Files with decode errors:\", ecount)\n",
    "    print(\"Files whose sources intersect with the files we have collected\", len(set(sl).intersection(ids)))\n",
    "    print (\"Total files\", len(ids))\n",
    "    news_df = pandas.DataFrame(columns=[\"title\", \"date\", \"country\", \"source\", \"url\", \"text\", \"tokens\",\"normalized_tokens\"])\n",
    "    count=0\n",
    "    sl = []\n",
    "    for news in sources:\n",
    "        try:\n",
    "            tid, total_words, date, country, source, url, title = news.decode(\"utf-8\").split(\"\\t\")\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "        try:\n",
    "            news_df.loc[tid.strip()] = [title.strip(), date.strip(), country.strip(), source.strip(), url.strip(), just_texts[tid.strip()], news_texts[tid.strip()][0],news_texts[tid.strip()][1]]\n",
    "            sl.append(tid)\n",
    "            count+=1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    #the tids not found in sources\n",
    "    for tids in list(ids.difference(set(sl))):\n",
    "        contents = ([\"\"]+[file_name.replace('.zip','').replace('text-','')+'-00'])+['US']+ ([\"\"]*2)+ [just_texts[tids], news_texts[tids][0],news_texts[tids][1]]\n",
    "        #print ((contents))\n",
    "        #print (type(contents))\n",
    "        #try:\n",
    "        news_df.loc[tids.strip()] = contents\n",
    "        #except:\n",
    "            #print (tids)\n",
    "    print('Shape of data_frame', news_df.shape)\n",
    "    print('Size of keys', len(ids))\n",
    "    news_df.to_pickle(file_name.replace('text-','20').replace('.zip','')+'_news_df.pkl')\n",
    "    print('File saved at',file_name.replace('text-','20').replace('.zip','')+'_news_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "<class 'str'> /project2/soci40133/Davies_corpora/NOW/text-16-12.zip\n",
      "3092\n",
      "File_name text_16-12-US.txt\n",
      "0.2551898956298828\n",
      "Processed 0\n",
      "14.6622314453125\n",
      "Processed 100\n",
      "11.963776111602783\n",
      "Processed 200\n",
      "12.512112379074097\n",
      "Processed 300\n",
      "13.05117917060852\n",
      "Processed 400\n",
      "12.340938091278076\n",
      "Processed 500\n",
      "12.661717414855957\n",
      "Processed 600\n",
      "11.571814060211182\n",
      "Processed 700\n",
      "14.099770545959473\n",
      "Processed 800\n",
      "12.132935523986816\n",
      "Processed 900\n",
      "13.307594776153564\n",
      "Processed 1000\n",
      "11.71561574935913\n",
      "Processed 1100\n",
      "12.390451669692993\n",
      "Processed 1200\n",
      "11.071134805679321\n",
      "Processed 1300\n",
      "17.184241771697998\n",
      "Processed 1400\n",
      "15.59457802772522\n",
      "Processed 1500\n",
      "13.074754238128662\n",
      "Processed 1600\n",
      "15.33001160621643\n",
      "Processed 1700\n",
      "14.212183952331543\n",
      "Processed 1800\n",
      "13.374427795410156\n",
      "Processed 1900\n",
      "13.36827826499939\n",
      "Processed 2000\n",
      "12.171555995941162\n",
      "Processed 2100\n",
      "14.62836766242981\n",
      "Processed 2200\n",
      "17.68419909477234\n",
      "Processed 2300\n",
      "13.861583948135376\n",
      "Processed 2400\n",
      "15.863219976425171\n",
      "Processed 2500\n",
      "14.574506998062134\n",
      "Processed 2600\n",
      "15.82218623161316\n",
      "Processed 2700\n",
      "13.733592510223389\n",
      "Processed 2800\n",
      "13.5648832321167\n",
      "Processed 2900\n",
      "18.602951288223267\n",
      "Processed 3000\n",
      "Total time 431.9911916255951\n",
      "Total source documents 269357\n",
      "Files with decode errors: 1967\n",
      "Files whose sources intersect with the files we have collected 3085\n",
      "Total files 3092\n",
      "Shape of data_frame (3092, 8)\n",
      "Size of keys 3092\n",
      "File saved at 2016-12_news_df.pkl\n",
      "Processed  1 of  2\n",
      "<class 'str'> /project2/soci40133/Davies_corpora/NOW/text-16-11.zip\n",
      "4852\n",
      "File_name text_16-11-US.txt\n",
      "0.35186171531677246\n",
      "Processed 0\n",
      "12.719557523727417\n",
      "Processed 100\n",
      "12.483468055725098\n",
      "Processed 200\n",
      "12.187511920928955\n",
      "Processed 300\n",
      "13.52616024017334\n",
      "Processed 400\n",
      "12.098084449768066\n",
      "Processed 500\n",
      "13.097788095474243\n",
      "Processed 600\n",
      "12.300659656524658\n",
      "Processed 700\n",
      "14.7486732006073\n",
      "Processed 800\n",
      "12.607326745986938\n",
      "Processed 900\n",
      "11.843905210494995\n",
      "Processed 1000\n",
      "10.049338579177856\n",
      "Processed 1100\n",
      "16.953144788742065\n",
      "Processed 1200\n",
      "11.678733587265015\n",
      "Processed 1300\n",
      "11.169753074645996\n",
      "Processed 1400\n",
      "11.08152985572815\n",
      "Processed 1500\n",
      "10.706175088882446\n",
      "Processed 1600\n",
      "9.689917087554932\n",
      "Processed 1700\n",
      "11.075135946273804\n",
      "Processed 1800\n",
      "12.48053789138794\n",
      "Processed 1900\n",
      "12.445513248443604\n",
      "Processed 2000\n",
      "11.345467329025269\n",
      "Processed 2100\n",
      "13.034094095230103\n",
      "Processed 2200\n",
      "10.143739938735962\n",
      "Processed 2300\n",
      "12.300854444503784\n",
      "Processed 2400\n",
      "13.197789907455444\n",
      "Processed 2500\n",
      "11.379707336425781\n",
      "Processed 2600\n",
      "12.17511510848999\n",
      "Processed 2700\n",
      "13.175237894058228\n",
      "Processed 2800\n",
      "11.032864809036255\n",
      "Processed 2900\n",
      "12.650801420211792\n",
      "Processed 3000\n",
      "12.264039516448975\n",
      "Processed 3100\n",
      "12.864881038665771\n",
      "Processed 3200\n",
      "13.715099334716797\n",
      "Processed 3300\n",
      "10.559150218963623\n",
      "Processed 3400\n",
      "14.602349281311035\n",
      "Processed 3500\n",
      "11.08548378944397\n",
      "Processed 3600\n",
      "11.0373055934906\n",
      "Processed 3700\n",
      "11.7227623462677\n",
      "Processed 3800\n",
      "14.031582593917847\n",
      "Processed 3900\n",
      "12.89638066291809\n",
      "Processed 4000\n",
      "13.149900197982788\n",
      "Processed 4100\n",
      "11.962562322616577\n",
      "Processed 4200\n",
      "12.437245607376099\n",
      "Processed 4300\n",
      "14.382689952850342\n",
      "Processed 4400\n",
      "12.117807388305664\n",
      "Processed 4500\n",
      "12.737743377685547\n",
      "Processed 4600\n",
      "12.706008195877075\n",
      "Processed 4700\n",
      "11.984090805053711\n",
      "Processed 4800\n",
      "Total time 600.2200071811676\n",
      "Total source documents 269379\n",
      "Files with decode errors: 1797\n",
      "Files whose sources intersect with the files we have collected 4843\n",
      "Total files 4852\n",
      "Shape of data_frame (4852, 8)\n",
      "Size of keys 4852\n",
      "File saved at 2016-11_news_df.pkl\n",
      "Processed  2 of  2\n"
     ]
    }
   ],
   "source": [
    "corpus_name = \"/project2/soci40133/Davies_corpora/NOW\"\n",
    "df_count = 0\n",
    "print (len(s_dict.keys()))\n",
    "for f in list(s_dict.keys()):\n",
    "    file_name=f\n",
    "    source_name=s_dict[file_name]\n",
    "    news_raw=loadDavies(corpus_name, file_name)\n",
    "    Process(news_raw,file_name,source_name)\n",
    "    df_count+=1\n",
    "    print('Processed ',df_count,'of ',len(s_dict.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(article_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids=set((news_texts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source matching\n",
    "sources = []\n",
    "zfile = zipfile.ZipFile(corpus_name + \"/\"+source_name)\n",
    "\n",
    "for file in zfile.namelist():\n",
    "    with zfile.open(file) as f:\n",
    "        for line in f:\n",
    "            sources.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl=[]\n",
    "ecount=0\n",
    "for s in sources:\n",
    "    try:\n",
    "        sl.append(s.decode().split(\"\\t\")[0].strip())\n",
    "        #print(s.decode().split(\"\\t\")[2].strip())\n",
    "    except UnicodeDecodeError:\n",
    "        ecount+=1\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total source documents\", len(sources))\n",
    "print(\"Files with decode errors:\", ecount)\n",
    "print(\"Files whose sources intersect with the files we have collected\", len(set(sl).intersection(ids)))\n",
    "print (\"Total files\", len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#news_texts[list(set(sl).intersection(ids))[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pandas.DataFrame(columns=[\"title\", \"date\", \"country\", \"source\", \"url\", \"text\", \"tokens\",\"normalized_tokens\"])\n",
    "count=0\n",
    "sl = []\n",
    "for news in sources:\n",
    "    try:\n",
    "        tid, total_words, date, country, source, url, title = news.decode(\"utf-8\").split(\"\\t\")\n",
    "    except UnicodeDecodeError:\n",
    "        continue\n",
    "    try:\n",
    "        news_df.loc[tid.strip()] = [title.strip(), date.strip(), country.strip(), source.strip(), url.strip(), just_texts[tid.strip()], news_texts[tid.strip()][0],news_texts[tid.strip()][1]]\n",
    "        sl.append(tid)\n",
    "        count+=1\n",
    "    except KeyError:\n",
    "        continue\n",
    "#the tids not found in sources\n",
    "for tids in list(ids.difference(set(sl))):\n",
    "    contents = ([\"\"]*2)+['US']+ ([\"\"]*2)+ [just_texts[tids], news_texts[tids][0],news_texts[tids][1]]\n",
    "    #print ((contents))\n",
    "    #print (type(contents))\n",
    "    #try:\n",
    "    news_df.loc[tids.strip()] = contents\n",
    "    #except:\n",
    "        #print (tids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check\n",
    "#news_df.loc['15159083']['tokens'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#news_df.to_csv('2016-11_news_df.csv')\n",
    "news_df.to_csv(file_name.replace('text-','20').replace('.zip','')+'_news_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
